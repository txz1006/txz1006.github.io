研究内容

1.门面框架slf4j、JPA的学习(解耦兼容层)
2.spring事务管理
	spring事务解析：  https://blog.csdn.net/justloveyou_/article/details/73733278

3.netty实例(已完成)
4.RSA加解密(已完成)
5.TCP抓包实战(进行中)
6.大文件上传(已完成)

7.OAuth2认证机制与开发方平台的优化方案
8.全链路监控的原理与实现
9.Lottery抽奖系统的设计与实现学习


===============================================================================
2021.10.12

---
在使用线程池时，需要注意任务和任务之间是否有先后等待关系，如CompletableFuture.join，如果存在多个任务都计算出结果时才能一起返回，当等待队列Queue容量大于运行时线程时，就会出现占用线程池资源的线程在join等待其他任务执行完成，而处于等待任务中的任务一直拿不到线程池资源，这样就形成死锁了。(解决：将Queue容量<=工作线程数，而且任务来源唯一)

装饰器对象，拥有和目标对象一些的方法列表(只多不少)，对目标方法只做功能增强(可以理解为aop，包裹执行目标方法)


2021.10.14
===
文件系统：

构成：文件系统一般由innode区和block区组成
文件管理模式：为了保证空间的充分利用率，磁盘空间的使用采用block块的形式紧密排列，每个block的空间大小是4K，地址大小是4B，如果存储文件大于4K，则会对文件进行拆分，存储到多个block中，并将文件的完整信息记录到innode区中，innode区中的每个元素会记录文件的名称，文件实际存储的block地址列表等信息。

文件存储规则：innode区的元素存储着文件元信息和block数组(长度为15),block数组的前12个元素可以指向12个block的地址，也就是存储空间为4K*12=48K；而第13个元素是一级索引，也就是这个元素会使用一个block块来存储其他block的地址，这样就等于这个元素一共存储了4096B(4K)/4B=1024个block的地址，计算空间为4K*1024=4M；同理第14个元素是二级索引，他在一级索引的1024个地址基础上，让这1024个地址指向的还是索引地址，也就是这个元素可以记录1024*1024个block的地址，计算空间为4K*1024*1024=4G；同样的做法，最后一个第15个元素可以存储4K*1024*1024*1024=4T的数据。总结一下就是，这个文件系统最大可存储的单文件大小为48K+4M+4G+4T


文件寻址：清楚文件存储方式，后我们就清楚一个文件最少的寻址需要两次，innode元素中block地址--->block读取；最大需要5次：innode元素中block地址--->一级索引地址--->二级索引地址--->三级索引地址--->block读取；


所以文件在文件系统存储时，size的大小，和实际的IO block大小是不同的，size指文件的表象大小，IO block指实际物理存储的block块空间

---------
controller层--->service层---->manager层--->dao层
在service层和dao层之间可以增加一个manager层，用于当service层逻辑过于复杂时，根据业务将不同功能拆分到manager层中，这样就可以分别对各个manager添加事务，没必要对整个service加事务了，有效的缩短事务执行时间。


2021.10.25
-源码阅读
- 源码解析：将整个系统拆分为多个功能点，每个功能拆分为多个对象，每个对象出一系列 逻辑类图+时序图，说明白这个对象的生存周期、主要作用、是否用到设计模式
- 源码应用：系统功能如何整合到开发项目中，怎么使用这些功能实战应用

----------
究竟是哪些元素吸引了我？(游戏中、购物中、娱乐中)
我为什么喜欢这些东西？(兴趣？猎奇？尝鲜？)
能不能吧这些元素复刻到正反馈系统中？



2021.11.18
LongAdder在高并发情况想性能优于AtomicLong的原因是：LongAdder在没有并发的情况下会只维护一个base值；有并发的情况下线程CAS成功则修改base值，CAS失败则创建Cell对象放入cell数组中，最后的和是base+cell数组的总和。这是属于典型的空间换时间做法


2021.11.23
------
主要工作一定制定计划
以计划目标作为工作导向
每个时间段需要反馈进展
善于提问保持谦逊


----------
mysql事务等级中RR级别和RC级别的区别：
RR级别下，mysql除了有行锁外，还会有间隙锁和第三种锁Next-key lock,这个锁是行锁的和间隙锁的组合，会开启两个间隙锁，即操作数据所在的区间和相邻的下个数据区间都会被加锁，参考：https://www.cnblogs.com/zhoujinyi/p/3435982.html
RC级别下只有行锁，所以RC级别下出现事务等待和死锁的概率会更小一些，如果并发过高可以考虑使用此级别事务等级



2021.11.26
---------
1.如果遇到数值类统计功能，如签到、点赞、记录使用天数，统计活跃用户等功能(用户id最好为整形)，可以使用redis的bit系列命令来完成，不仅占内存极少，而且效率高
2.对于计算连续签到的功能，可以使用redis的bit位图来存储数据，一般而言每个位图的key是sign:2021:userId,只存储一年的数据，而使用MMdd月天作为偏移量，如果用户进行了签到，就将这个偏移量的值设置为1，
那么我们得到的数据就是1101111110111001111这样的一个bit位的二进制数
这样在计算连续签到的功能可以使用
        for (int i = 0; i < date.getDayOfMonth(); i++) {
            //如果是连续签到得到的long值右移一位再左移一位后与原始值不相等,连续天数加一
            if (v >> 1 << 1 == v) return signCount;
            signCount += 1;
            v >>= 1;
        }
来判断从右向左右几个连续的1, 满足这个等式则代表遇到了0就终止累加，

假如v = 11011 
则有 
11010 == 11011
v >>=1 等于 5505 

5504 == 5505
v >>=1 等于 2752

2752 == 2752 跳出循环
这样signCount就只累加了2次，就是最右边的2位1

如果想计算有多少天没有签到，可以将条件改为v >> 1 << 1 != v,可以累加计算最右边有多少个连续的0

3.bitmap对于大数据量的统计等功能可以节约很多内存空间，但注意一点，如果需要维护的数据量比较小，offset却比较大，这样反而会浪费很多内存

4.使用bitmap开发点赞功能，可以一个帖子创建一个bitmap对象，根据userid进行offset匹配设值，如果userid很大，可以设置一种转换关系，把offset变成比较小的值，
如：userid=500001，我们可以在设置bitmap时，让userid-500000 = 1,来设置offset

----

2021.12.01
单点登录cas：
1.任何系统在登录时会重定向到CAS服务器，CAS服务器中维护了一套完整的用户权限关系
2.在CAS服务器中登录后，会重定向返回到登录的系统当中，此时会携带CAS服务器给登录用户生成的token
3.登录子系统拿到token后，会去CAS服务器验证token对应的登录用户信息，通过后拿userid一类的数据在当前子系统执行(注册)登录操作




2021.12.09
序列化就是让内存对象变得可以进行传输或者存储，而序列化后的形式可以是字符串(JSON、xml、Protobuf一类格式),字节数组等，在各种网络请求中都可以看到序列化和反序列化的应用

问题：序列化和编码都是把 Java 对象封装成二进制数据的过程，这两者有什么区别和联系？
- 序列化是让内存对象变的可传输存储
- 编码是让编程语言代码变得可以被计算机读懂


- 如何将对象转换成字节数组，就是使用不同的方式进行序列化
方式一：JAVA序列化，对象实现serializabel接口，并使用ObjectOutputStream.writeObject(obj)转化，而ObjectOutputStream的输出位置可以是文件(FileOutputStream)也可以是二进制字节数组(ByteArrayOutputStream)
//一般不建议使用JAVA序列化方式，因为这种方式无法跨语言解析使用，而且生成的字节流比较大，相比其他的方式
//补充内容：如果对象中某些字段不想参与序列化，则可以使用transient修饰； 
//一般而言，通常需要显示指定一个序列化版本号serialVersionUID，不指定的话虽然会自动分配，但是只要文件发生改动，这个版本号值就会变化，容易导致反序列化失败

方式二：JSON格式，以fastjson为例，使用JSON.toJSONBytes(obj)转换为字节数组，使用JSON.parseObject(bytes,clazz)转换为对象；或者字符串类型JSON.toJSONString(obj)和JSON.parseObject(str, clazz)
方式三：Protobuf格式，


2021.12.10
- 对于动态数据流而言，最简单的方式就是使用一个定时任务记录时序数据，之后使用echarts一类的图表渲染


ctx.channel().writeAndFlush()可以给客户端、服务端发送数据(每个pipeline元素都可以)
ctx.fireChannelRead()可以触发下一个pipeline的执行

MessageToByteEncoder对象会在outBound链表执行完之后进行二进制编码(?)
ByteToMessageDecoder对象会在inBound链表执行前进行二进制对象转换(?)





3.slice() 方法与 duplicate() 方法的相同点是：底层内存以及引用计数与原始的 ByteBuf 共享，也就是说经过 slice() 或者 duplicate() 返回的 ByteBuf 调用 write 系列方法都会影响到
原始的 ByteBuf，但是它们都维持着与原始 ByteBuf 相同的内存引用计数和不同的读写指针 
4. slice() 方法与 duplicate() 不同点就是：slice() 只截取从 readerIndex 到 writerIndex 之间的数据，它返回的 ByteBuf 的最大容量被限制到 原始 ByteBuf 的 readableBytes(), 而 duplicate() 是把整个 ByteBuf 都与原始的 ByteBuf 共享 
5. slice() 方法与 duplicate() 方法不会拷贝数据，它们只是通过改变读写指针来改变读写的行为，而最后一个方法 copy() 会直接从原始的 ByteBuf 中拷贝所有的信息，包括读写指针以及底层对应的数据，因此，往 copy() 返回的 ByteBuf 中写数据不会影响到原始的 ByteBuf 
6. slice() 和 duplicate() 不会改变 ByteBuf 的引用计数，所以原始的 ByteBuf 调用 release() 之后发现引用计数为零，就开始释放内存，调用这两个方法返回的 ByteBuf 也会被释放，这个时候如果再对它们进行读写，就会报错。因此，我们可以通过调用一次 retain() 方法 来增加引用，表示它们对应的底层的内存多了一次引用，引用计数为2，在释放内存的时候，需要调用两次 release() 方法，将引用计数降到零，才会释放内存 
7. 这三个方法均维护着自己的读写指针，与原始的 ByteBuf 的读写指针无关，相互之间不受影响

==================================================================================
==================================================================================

redis类型的应用：

String类型：1.用来存储单字符串  2.存储json等序列化格式数据   3.存储计数器一类数据

hash类型：1.存储对象   (为什么不用string，用string存储需要有序列化开销；如果只要获取一两个字段没必要获取整个对象)

list类型： 1.对数据集合进行增减分页等     2.消息队列

set类型：去重的list，无序

zset类型： 有序的set

========================


注意maven依赖设置的不对，可能导致profile失效

可以在maven根pom文件中定义正常的依赖控制版本号，在子模块中的pom文件中引用同一个依赖时可以不携带版本号


为什么多数数据库连接池不采用 IO 多路复用？
答：历史与生态决定，jdbc访问db是同步阻塞的，而且改成异步请求并不会提供太多的性能，反而提高了复杂度







