### CommitLog和CustomerQueueLog是基于OS cache+磁盘一起读取的

通过前面的学习，我们知道RocketMQ的数据一般是通过OS PageCache异步持久化到磁盘文件的，那么在消费者读取数据时，不可能一直去磁盘读取数据信息。更何况当消费者数量过多时，CommitLog和CustomerQueueLog的数据读取会非常的频繁，如何每次都要通过磁盘IO读取数据是不可能有高性能的。

那么RocketMQ是如何设计保证读取的高性能的呢？答案是将需要频繁读取的数据在首次加载时，放到OS cache中，这样在后续的读取中就会直接从内存中读取数据，以此来保证高吞吐量。

一般而言，ConsumeQueue文件主要是存放消息的offset，所以每个文件很小，30万条消息的offset就只有5.72MB而已。所以实际上ConsumeQueue文件们是不占用多少磁盘空间的，他们整体数据量很小，几乎可以完全被os缓存在内存cache里。

![image-20230324152406629](https://alex-img-1253982387.cos.ap-nanjing.myqcloud.com/Typora-wm/202303241524872.png)

#### **什么时候会从os cache读？什么时候会从磁盘读？**

无论是CommitLog还是CustomerQueueLog，那么什么情况会从os PageCache读取？什么时候又从磁盘读取呢？一般来说都有可能，具体来说和消费者消费数据的及时性有关，os PageCache空间毕竟有效，一个CommitLog最大能有1个G，不能降所有数据一直放在内存中，所以os cache一般会存储最新的数据，空间不够用时，会按时间顺序清除掉旧的内存数据，这个数据再要读取就只能通过磁盘文件来读取。

所以根据这个规律，如果消费者消费数据都非常的及时，那么这些数据可能都在MQ的OS cache中，就可以很快的进行内存寻址读取，然后返回。

如果MQ在短时间内积压了大量的数据，消费者消费不过来，那么时间靠前的数据可能就在磁盘文件中了，不在OS PageCache中，这种情况就需要进行磁盘IO来读取数据到内存中来进行消费。

### **Master Broker什么时候会让你从Slave Broker拉取数据？**

一般而言，正常拉取会去Master节点拉取数据，但是如果Master Broker节点积压了过多数据，但是消费者一直还没有消费这个数据，那么Master节点就会告诉消费者下次去Slave节点拉取数据。

这个过程中的关键点是看Master Broker节点积压了多少还未消费的数据，如果积压的数据量超过了OS PageCache的全部内存，那么就会触发这个机制。例如如果一个Topic有10万数据还未消费，但是OS PageCache最大能存储的数据量只有8万，也就是说至少有2万数据是一定在磁盘中存储的；这种情况下Master Broker节点还会处理本次数据请求，但是会通知消费者下一次去Slave节点拉取数据。

本质是对比你当前没有拉取消息的数量和大小，以及最多可以存放在os PageCache内存里的消息的大小，如果你没拉取的消息超过了最大能使用的内存的量，那么说明你后续会频繁从磁盘加载数据，此时就让你从slave broker去加载数据了！



**（1）消费者机器到底是跟少数几台Broker建立连接，还是跟所有Broker都建立连接？这是不少朋友之前在评论区提出的问题，但是我想这里大家肯定都有自己的答案了。**

消费者只会和自己对应要拉取消息的topic下的messagequeue所在的broker节点建立连接



**（2）RocketMQ是支持主从架构下的读写分离的，而且什么时候找Slave Broker读取大家也都了解的很清楚了，那么大家思考一下，Kafka、RabbitMQ他们支持主从架构下的读写分离吗？支持Slave Broker的读取吗？为什么呢？**

kafka是支持主从架构下读写分离的，rabbitmq并没有读写分离，它的集群模式是per-per的，每个节点都支持读写，然后把数据同步给其他节点。



**（3）如果支持读写分离的话，有没有一种可能，就是出现主从数据不一致的问题？比如有的数据刚刚到Master Broker和部分Slave Broker，但是你刚好是从那个没有写入数据的Slave Broker去读取了？**

kafka和rabbitmq都存在数据不一致的情况。



**（4）消费吞吐量似乎是跟你的处理速度有很大关系，如果你消费到一批数据，处理太慢了，会导致你严重跟不上数据写入的速度，这会导致你后续几乎每次拉取数据都会从磁盘上读取，而不是os cache里读取，所以你觉得你在拉取到一批消息处理的时候，应该有哪些要点需要注意的？**

①尽量保证消费者和生产者的速率一致。 ②消费速度太慢，可以使用多线程处理 ③多开几个消费者，假如说消费者和生产者的速率为1:4，那么可以使用4个消费者，一个生产者，来达到平衡



### RocketMQ是如何基于Netty实现高性能网络通信架构的？

首先，Netty是基于NIO和Reactor模型实现的一个高性能网络通信框架，RocketMQ的网络通信性能依赖Netty的架构和配置，以及后续自己扩展的部分。

Netty的通信流程是这样的：

- 首先Netty会有一个Reactor主线程监听一个端口信息，用来接受请求信息，会有Producer请求MQ的监听端口建立连接
- 然后Netty和请求程序会使用ScoketChannel构建一个长连接，Netty会将这个ScoketChannel对象交给Reactor线程池中的一个线程处理
- Reactor线程池中的线程接受完信息后，会将请求数据交给work线程池进一步做业务处理，而且在业务处理之前，还会进行一些请求预处理，比如SSL加解密，编码解码，连接空闲检查等工作。
- work线程池处理完成后，会将格式规范的请求数据交给MQ的业务线程池进行处理，比如之前学习过的MQ信息存入CommitLog日志过程。

### mmap零拷贝技术如何实现的Page Cache缓存功能？

前面的学习中，我们知道CommitLog等日志数据的持久化是基于Page Cache系统内存实现的，那么这个Page Cache到底是什么呢？

在进程使用系统资源的流程中，是不直接可以去使用的，这个过程需要受到操作系统的限制，具体而言就是，MQ应程属于用户态进程，系统资源的访问都需要通过操作系统提供的内核态函数来间接使用资源。由于有操作系统这么个中间人，所以从网卡和磁盘读取数据，都需要经过操作系统内核态的IO缓冲区的转手，才能到用户态进程手中。数据进行了两次复制：

- 磁盘复制到内核态IO缓冲区
- 内核态IO缓冲区复制到进程的内存中

![image-20230327172954947](https://alex-img-1253982387.cos.ap-nanjing.myqcloud.com/Typora-wm/202303271730344.png)

而mmap零拷贝技术就是对于这种流程的一个优化，他会通过一个地址映射创建出一个横跨用户态和内核态的一片内存空间，这个空间由用户进程和操作系统共享，然后将磁盘数据复制到这个空间中给用户进程使用。

这个地址映射的过程，就是JDK NIO包下的MappedByteBuffer.map()函数干的事情，底层就是基于mmap技术实现的。

![image-20230327175326279](https://alex-img-1253982387.cos.ap-nanjing.myqcloud.com/Typora-wm/202303271753319.png)

这种基于mmap零拷贝技术实现的文件读取方式只需要一次复制就可以读取文件：磁盘文件复制到虚拟内存中(PageCahe)，用户进程直接从虚拟内存读取使用文件数据。

而RocketMQ在此基础上做了**预映射**和**预读取**的工作：

- 预映射，Broker会将接下来需要读取的CommitLog日志和CustomerQueueLog文件完成mmap的地址映射。
- 预读取，提前对一些文件完成映射之后，因为映射不会直接将数据加载到内存里来，那么后续在读取尤其是CommitLog、ConsumeQueue的时候，其实有可能会频繁的从磁盘里加载数据到内存中去。
